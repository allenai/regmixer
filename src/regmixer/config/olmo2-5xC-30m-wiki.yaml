name: "olmo2-5xC-30m"
description: "OLMoE-mix-0824 30M @ 5xC scale"
budget: "ai2/oe-data"
workspace: "ai2/dolma2"
nodes: 1
gpus: 8 # 1, 8
variants: 32 #64 # 1, 5, 100
preemptible: true
max_tokens: 2_910_233_600
allow_repetition: false 
sequence_length: 4096
seed: 42
mix_temperature: 0.7
minimum_weight: 0.0001
#min_strength: 1
max_strength: 20
proxy_model_id: "olmo_30m"
tokenizer: "dolma2"
weka: true
dtype: "uint32"
priority: high #high #normal, high
cluster: ai2/saturn-cirrascale #ai2/saturn-cirrascale, ai2/neptune-cirrascale
sources:
  - name: proofpile-2-stack
    paths:
      - s3://ai2-llm/preprocessed/proof-pile-2/v0_decontaminated/algebraic-stack/train/allenai/dolma2-tokenizer/*.npy
  - name: proofpile-2-arxiv
    paths:
      - s3://ai2-llm/preprocessed/proof-pile-2/v0_decontaminated/arxiv/train/allenai/dolma2-tokenizer/*.npy
  - name: proofpile-2-open-web-math
    paths:
      - s3://ai2-llm/preprocessed/proof-pile-2/v0_decontaminated/open-web-math/train/allenai/dolma2-tokenizer/*.npy
  - name: pes2o
    paths:
      - s3://ai2-llm/preprocessed/pes2o/allenai/dolma2-tokenizer/*.npy
  - name: starcoder
    paths:
      - s3://ai2-llm/preprocessed/starcoder/v1-decon-100_to_20k-2star-top_token_030/allenai/dolma2-tokenizer/*.npy
  - name: dclm
    paths:
      - s3://ai2-llm/preprocessed/dclm/text_openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train/allenai/dolma2-tokenizer/*.npy
  - name: wikipedia
    paths:
      - s3://ai2-llm/preprocessed/olmo-mix/danyh-compiled-v1_7/documents/wiki/allenai/dolma2-tokenizer/*.npy
nonzero_weight:
  - wikipedia