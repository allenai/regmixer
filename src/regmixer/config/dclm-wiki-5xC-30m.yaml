name: "dclm-wiki-5xC-30m"
description: "dclm + wiki mix 30M @ 5xC scale"
budget: "ai2/oe-data"
workspace: "ai2/dolma2"
nodes: 1
gpus: 8 # 1, 8
variants: 16 #64 # 1, 5, 100
preemptible: true
max_tokens: 2_910_233_600
allow_repetition: false 
sequence_length: 4096
seed: 42
mix_temperature: 0.4
minimum_weight: 0.01
#min_strength: 1
max_strength: 20
proxy_model_id: "olmo_30m"
tokenizer: "dolma2"
weka: true
dtype: "uint32"
priority: high #high #normal, high
cluster: ai2/saturn-cirrascale #ai2/saturn-cirrascale, ai2/neptune-cirrascale
device_batch_size: 4
sources:
  - name: dclm
    paths:
      - s3://ai2-llm/preprocessed/dclm/text_openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train/allenai/dolma2-tokenizer/*.npy
  - name: wikipedia
    paths:
      - s3://ai2-llm/preprocessed/olmo-mix/danyh-compiled-v1_7/documents/wiki/allenai/dolma2-tokenizer/*.npy