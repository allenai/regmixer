#!/bin/bash
#rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-65-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=65 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_65_samples_seed_1.yaml"
#rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-119-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=119 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_119_samples_seed_2.yaml"
#rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-14-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=14 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_14_samples_seed_0.yaml"
#rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-7-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=7 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_7_samples_seed_1.yaml"
#rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-70-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=70 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_70_samples_seed_0.yaml"
#rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-38-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=38 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_38_samples_seed_1.yaml"
#rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-70-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=70 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_70_samples_seed_2.yaml"

rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-91-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=91 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_91_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-26-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=26 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_26_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-105-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=105 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_105_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-42-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=42 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_42_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-7-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=7 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_7_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-70-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=70 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_70_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-117-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=117 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_117_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-39-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=39 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_39_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-49-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=49 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_49_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-78-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=78 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_78_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-28-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=28 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_28_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-117-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=117 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_117_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-57-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=57 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_57_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-95-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=95 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_95_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-21-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=21 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_21_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-19-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=19 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_19_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-14-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=14 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_14_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-98-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=98 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_98_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-117-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=117 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_117_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-104-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=104 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_104_samples_seed_2.yaml"

#rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-91-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=91 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_91_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-84-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=84 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_84_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-104-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=104 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_104_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-114-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=114 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_114_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-105-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=105 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_105_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-78-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=78 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_78_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-114-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=114 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_114_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-13-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=13 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_13_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-63-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=63 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_63_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-130-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=130 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_130_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-84-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=84 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_84_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-65-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=65 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_65_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-119-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=119 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_119_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-57-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=57 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_57_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-52-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=52 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_52_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-38-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=38 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_38_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-78-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=78 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_78_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-112-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=112 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_112_samples_seed_0.yaml"

#rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-98-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=98 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_98_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-38-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=38 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_38_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-76-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=76 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_76_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-14-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=14 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_14_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-119-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=119 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_119_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-52-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=52 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_52_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-77-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=77 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_77_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-98-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=98 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_98_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-19-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=19 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_19_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-35-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=35 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_35_samples_seed_2.yaml"

#rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-26-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=26 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_26_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-112-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=112 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_112_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-49-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=49 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_49_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-112-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=112 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_112_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-105-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=105 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_105_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-84-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=84 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_84_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-57-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=57 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_57_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-42-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=42 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_42_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-52-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=52 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_52_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-77-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=77 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_77_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-19-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=19 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_19_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-91-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=91 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_91_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-77-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=77 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_77_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-76-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=76 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_76_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-65-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=65 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_65_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-26-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=26 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_26_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-130-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=130 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_130_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-91-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=91 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_91_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-7-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=7 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_7_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-56-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=56 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_56_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-56-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=56 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_56_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-104-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=104 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_104_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-56-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=56 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_56_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-63-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=63 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_63_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-114-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=114 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_114_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-95-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=95 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_95_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-39-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=39 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_39_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-35-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=35 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_35_samples_seed_0.yaml"

#rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-28-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=28 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_28_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-39-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=39 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_39_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-49-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=49 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_49_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-13-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=13 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_13_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-130-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=130 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_130_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-21-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=21 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_21_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-95-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=95 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_95_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-13-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=13 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_13_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-18-domains-lightgbm-76-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=18 domains, n=76 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_18_domains_lightgbm_76_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-91-samples-seed-0" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=91 samples, seed=0" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_91_samples_seed_0.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-21-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=21 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_21_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-42-samples-seed-2" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=42 samples, seed=2" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_42_samples_seed_2.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-28-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=28 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_28_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-12-domains-lightgbm-91-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=12 domains, n=91 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_12_domains_lightgbm_91_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-35-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=35 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_35_samples_seed_1.yaml"
rmc-internal train -t "dolma2" -c "ai2/augusta-google-1" -w "ai2/dolma2" -b "ai2/oe-base" -N 1 -g 1 -i "olmo_30m" -m 2_910_233_600 -l 2048 -D uint32 -S 42 -p high --device-batch-size 32 -n "cost-ablation-top-6-domains-lightgbm-63-samples-seed-1" -d "5xC 30M DCLM proposed LightGBM mix (rep=4) with d=6 domains, n=63 samples, seed=1" -s "for_paper/cost_ablation_lightgbm/top_6_domains_lightgbm_63_samples_seed_1.yaml"
